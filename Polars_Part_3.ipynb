{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvvEvKzsWboV"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bettercodepaul/data-wrangling-praktikum/blob/master/Polars_Part_3.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Polars: The Turbo Boost for Dataframes - Part 3\n",
    "\n",
    "Important links as a reminder:\n",
    "\n",
    "- Homepage of Polars: https://www.pola.rs/\n",
    "- User Guide: https://pola-rs.github.io/polars/user-guide/\n",
    "- API Reference: https://pola-rs.github.io/polars/py-polars/html/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EJKvLC6Z8ve"
   },
   "source": [
    "## Installation + Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utility module und data\n",
    "import urllib.request\n",
    "import os.path\n",
    "UTILS_URL = \"https://github.com/bettercodepaul/data-wrangling-praktikum/raw/master/utils.py\"\n",
    "urllib.request.urlretrieve(UTILS_URL, os.path.basename(UTILS_URL))\n",
    "from utils import download_data, download_region_data\n",
    "download_data()\n",
    "download_region_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars\n",
    "import polars as pl\n",
    "# output up to 60 characters per column and do not abbreviate floating point numbers\n",
    "pl.Config(fmt_str_lengths=60, fmt_float=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import exercises and utility functions\n",
    "from utils import *\n",
    "from exercises_en import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file and parse date columns\n",
    "df = pl.read_csv(\"spotify-charts-2017-2021-global-top200.csv.gz\", try_parse_dates=True)\n",
    "df.head(2) # output the first 2 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When.Then.Otherwise\n",
    "\n",
    "Sometimes you want to calculate an expression in certain cases like this and in other cases like that.\n",
    "\n",
    "For this there are the methods `when.then.otherwise`, which correspond to an `if.then.else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"spotify-charts-2017-2021-global-top200.csv.gz\", try_parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"date\").dt.day().eq(14) & pl.col(\"date\").dt.month().eq(2))\n",
    "        .then(pl.col(\"streams\"))\n",
    "        .otherwise(pl.lit(0))\n",
    "        .alias(\"valentinesStreams\")\n",
    "    )\n",
    "    .filter(pl.col(\"title\").eq(\"Starboy\") & pl.col(\"date\").dt.week().eq(7))\n",
    "    .select(\"date\", \"streams\", \"valentinesStreams\")\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also recreate the trend column ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    "    .join(\n",
    "        # Determine rank from previous day\n",
    "        df.select(\"url\", pl.col(\"date\").dt.offset_by(\"1d\"), pl.col(\"rank\").alias(\"previous_rank\")),\n",
    "        how=\"left\",\n",
    "        on=[\"url\", \"date\"]\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"rank\").lt(pl.col(\"previous_rank\")))\n",
    "        .then(pl.lit(\"MOVE_UP\"))\n",
    "        .otherwise(\n",
    "            pl.when(pl.col(\"rank\").gt(pl.col(\"previous_rank\")))\n",
    "            .then(pl.lit(\"MOVE_DOWN\"))\n",
    "            .otherwise(\n",
    "                pl.when(pl.col(\"rank\").eq(pl.col(\"previous_rank\")))\n",
    "                .then(pl.lit(\"SAME_POSITION\"))\n",
    "                .otherwise(pl.lit(\"NEW_ENTRY\"))\n",
    "            )\n",
    "        ).alias(\"myTrend\")\n",
    "    )\n",
    "    .select(\"title\", \"artist\", \"date\", \"trend\", \"myTrend\")\n",
    "    .sample(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is only about replacing certain, single values with others, `replace` can also be very handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"SAME_POSITION\": \"âž¡ï¸\",\n",
    "    \"NEW_ENTRY\": \"ðŸ†•\",\n",
    "    \"MOVE_UP\": \"â¬†ï¸\",\n",
    "    \"MOVE_DOWN\": \"â¬‡ï¸\"\n",
    "}\n",
    "(df\n",
    "  .with_columns(pl.col(\"trend\")\n",
    "  .replace(mapping).alias(\"trendSymbol\"))\n",
    "  .group_by(\"trendSymbol\")\n",
    "  .len()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please do not: `map_*`\n",
    "\n",
    "The worst possible way to smuggle your own functions into a Polars query are the various map methods `map_rows`, `map_batches`, `map_elements` and `map_groups`, which execute a UDF (User Defined Function).\n",
    "\n",
    "This should be avoided because performance suffers greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression Factories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more elegant way to get modular code is to use custom methods that create new expressions and can be called e.g. with the `pipe` method.\n",
    "\n",
    "Here is an example for the method `sum` for which there is no `min_count` parameter in Polars (but there is in Pandas). The `min_count` parameter determines how many values must be present at least for the sum to be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(expr: pl.Expr, min_count=0) -> pl.Expr:\n",
    "    if min_count > 0:\n",
    "        return pl.when(\n",
    "            expr.is_not_null().sum().ge(pl.lit(min_count))\n",
    "        ).then(expr.sum())\n",
    "    else:\n",
    "        return expr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct call to the method\n",
    "pl.DataFrame({\n",
    "    \"value\": [42, 43, None], \n",
    "}).select(\n",
    "    sum(pl.col(\"value\"), min_count=2).alias(\"min_count=2\"),\n",
    "    sum(pl.col(\"value\"), min_count=3).alias(\"min_count=3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call via pipe\n",
    "pl.DataFrame({\n",
    "    \"value\": [42, 43, None], \n",
    "}).select(\n",
    "    pl.col(\"value\").pipe(sum, min_count=2).alias(\"min_count=2\"),\n",
    "    pl.col(\"value\").pipe(sum, min_count=3).alias(\"min_count=3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also register such methods in a separate namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pl.api.register_expr_namespace(\"special\")\n",
    "class Special:\n",
    "    def __init__(self, expr: pl.Expr):\n",
    "        self._expr = expr\n",
    "\n",
    "    def sum(self, min_count=0) -> pl.Expr:\n",
    "        if min_count > 0:\n",
    "            return pl.when(\n",
    "                self._expr.is_not_null().sum().ge(pl.lit(min_count))\n",
    "            ).then(self._expr.sum())\n",
    "        else:\n",
    "            return self._expr.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the method within its own namespace \"special\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame({\n",
    "    \"value\": [42, 43, None], \n",
    "}).select(\n",
    "    pl.col(\"value\").special.sum(min_count=2).alias(\"min_count=2\"),\n",
    "    pl.col(\"value\").special.sum(min_count=3).alias(\"min_count=3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information is available here:\n",
    "\n",
    "- [Expr.pipe](https://pola-rs.github.io/polars/py-polars/html/reference/expressions/api/polars.Expr.pipe.html)\n",
    "- [DataFrame.pipe](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.pipe.html)\n",
    "- [Extending the API](https://pola-rs.github.io/polars/py-polars/html/reference/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6SrEeCgd3D0"
   },
   "source": [
    "## Lazy vs. Eager\n",
    "\n",
    "Until now we have always used Polars in \"eager mode\". Each function call directly resulted in an operation on the data.\n",
    "\n",
    "This has advantages when debugging queries, but prevents many optimizations that Polars can only use in \"lazy mode\".\n",
    "\n",
    "There are two options for the \"lazy mode\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager Load + Lazy Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a data set is not too large, we can load it completely into memory, as we already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"spotify-charts-2017-2021-global-top200.csv.gz\")\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `lazy` method we then switch to the \"lazy mode\". The execution of the query is now stopped and with each further call the query is only \"formulated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df = df.lazy()\n",
    "type(lazy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a lazy dataframe the unoptimized query tree is printed\n",
    "lazy_df.select(\"artist\", \"title\").filter(pl.col(\"artist\").eq(\"Dua Lipa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is read from bottom to top. The Greek letters are from relational algebra. The letter Ï€ stands for the operation projection (`select`), Ïƒ for the operation selection (`filter`).\n",
    "\n",
    "- Table Ï€ */9; Ïƒ -; means that all nine columns are read and no selection is made.\n",
    "- Ï€ 2/9 means that two out of nine columns are projected \n",
    "- FILTER BY is the filter from our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the method show_graph() we can output the optimized query\n",
    "lazy_df.select(\"artist\", \"title\").filter(pl.col(\"artist\").eq(\"Dua Lipa\")).show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both projection and selection happen earlier in the optimized query plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query is finally executed when we call the `collect` method. The result is then a normal dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lazy_df.select(\"artist\", \"title\").filter(pl.col(\"artist\").eq(\"Dua Lipa\")).collect()\n",
    "result.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, Polars can perform optimizations before executing the query.\n",
    "\n",
    "A selection of optimizations can be found here: https://pola-rs.github.io/polars/user-guide/lazy/optimizations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Load + Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not worth loading a data set completely into memory, we can also delay loading the data by using the IO methods named `scan_*` instead of `write_*`.\n",
    "\n",
    "This works e.g. for files in CSV (`scan_csv`) and Parquet (`scan_parquet`) formats, but not for compressed CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have always worked with a small dataset containing only the global top-200 charts (362k lines, 64 MB).\n",
    "\n",
    "We can now switch to the real dataset that contains the Top-200 and Viral-50 charts for 70 different regions (26m rows, 4 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet(\"spotify-charts-2017-2021.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the optimized queries, only the data that is really needed is loaded from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    "    .select(\"artist\", \"title\")\n",
    "    .filter(pl.col(\"artist\").eq(\"Dua Lipa\"))\n",
    ").show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the query, certain optimizations cannot be performed because they would change the result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    "    .head(2)\n",
    "    .select(\"artist\", \"title\")\n",
    "    .filter(pl.col(\"artist\").eq(\"Dua Lipa\"))\n",
    ").show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "If the final result or even intermediate results of a query do not fit into RAM anymore, Polars has a \"streaming mode\" which can significantly reduce the required RAM.\n",
    "\n",
    "If only the intermediate results are the problem, the streaming mode can be activated with `collect(streaming=True)`. The final result must then fit into the RAM.\n",
    "\n",
    "To write also a final result that does not fit into RAM anymore to disk, the methods `sink_parquet`, `sink_csv` and `sink_ipc` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Jupyter kernel crashed, restart and run this line\n",
    "import polars as pl\n",
    "df = pl.scan_parquet(\"spotify-charts-2017-2021.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction is the fraction of rows and influences the memory requirement\n",
    "# 0.003 ~ 4 GB (should run with 8 GB RAM)\n",
    "# 0.005 ~ 10 GB (should run with 16 GB RAM)\n",
    "# 0.008 ~ 26 GB (should run with 32 GB RAM)\n",
    "# 0.010 ~ 41 GB (should run with 64 GB RAM)\n",
    "# 0.015 ~ 92 GB (should run with 128 GB RAM)\n",
    "fraction = 0.008\n",
    "row_count = round(26173514*fraction)\n",
    "high_mem_query = (\n",
    "    df.head(row_count).join(df.head(row_count), on=\"artist\")\n",
    "    .filter(\n",
    "        pl.col(\"url\").ne(pl.col(\"url_right\")) &\n",
    "        pl.col(\"date\").gt(pl.col(\"date_right\")) &\n",
    "        pl.col(\"trend\").eq(\"NEW_ENTRY\") &\n",
    "        pl.col(\"trend_right\").eq(\"NEW_ENTRY\")\n",
    "    )\n",
    "    .group_by(\"url\").agg((pl.col(\"date\") - pl.col(\"date_right\")).min().alias(\"durationBetweenNewEntries\"))\n",
    "    .select(pl.col(\"durationBetweenNewEntries\").mean())\n",
    ")\n",
    "print(f\"Cross-product of {row_count:_} rows would contain {row_count**2:_} rows.\")\n",
    "print(f\"Estimated size for the intermediate join result is {6e-10*row_count**2:.2f} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different fractions with streaming=False and streaming=True\n",
    "high_mem_query.collect(streaming=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a query can be executed in streaming mode it will be inside a \"STREAMING\" node. If some nodes can not be streamed you will see them outside of the \"STREAMING\" node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mem_query.show_graph(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.scan_parquet(\"spotify-charts-2017-2021.parquet\")\n",
    "    .with_columns(pl.col(\"streams\").cast(pl.Int64))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q21.question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q21_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q21.check(q21_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q22.question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q22_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q22.check(q22_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q23.question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q23_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q23.check(q23_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q24.question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q24_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q24.check(q24_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25.question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = pl.scan_csv(\"region-info.csv\")\n",
    "xmasYears_per_continent = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25.check(q25_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
